{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RCNN Experiments.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "329tnhD4W-sK",
        "VtOgNL5zYmZl"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lthq9l2zVw82",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "4213a956-2a8a-42e8-8968-956af2041462"
      },
      "source": [
        "!pip install pmdarima\n",
        "!pip install pyts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pmdarima\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/e9/6587edeffba78fbed826c45d2e85edfba1fcb18f3d7d5347b20cdbdc7327/pmdarima-1.7.0-cp36-cp36m-manylinux1_x86_64.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (1.0.5)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (0.22.2.post1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (0.16.0)\n",
            "Collecting statsmodels>=0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cb/83/540fd83238a18abe6c2d280fa8e489ac5fcefa1f370f0ca1acd16ae1b860/statsmodels-0.11.1-cp36-cp36m-manylinux1_x86_64.whl (8.7MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7MB 25.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (1.18.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (1.4.1)\n",
            "Collecting Cython<0.29.18,>=0.29\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/d7/510ddef0248f3e1e91f9cc7e31c0f35f8954d0af92c5c3fd4c853e859ebe/Cython-0.29.17-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 41.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->pmdarima) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->pmdarima) (2.8.1)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.6/dist-packages (from statsmodels>=0.11->pmdarima) (0.5.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.19->pmdarima) (1.15.0)\n",
            "Installing collected packages: statsmodels, Cython, pmdarima\n",
            "  Found existing installation: statsmodels 0.10.2\n",
            "    Uninstalling statsmodels-0.10.2:\n",
            "      Successfully uninstalled statsmodels-0.10.2\n",
            "  Found existing installation: Cython 0.29.21\n",
            "    Uninstalling Cython-0.29.21:\n",
            "      Successfully uninstalled Cython-0.29.21\n",
            "Successfully installed Cython-0.29.17 pmdarima-1.7.0 statsmodels-0.11.1\n",
            "Collecting pyts\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/2b/1a62c0d32b40ee85daa8f6a6160828537b3d846c9fe93253b38846c6ec1f/pyts-0.11.0-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 3.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.6/dist-packages (from pyts) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.17.5 in /usr/local/lib/python3.6/dist-packages (from pyts) (1.18.5)\n",
            "Requirement already satisfied: numba>=0.48.0 in /usr/local/lib/python3.6/dist-packages (from pyts) (0.48.0)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from pyts) (0.16.0)\n",
            "Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from pyts) (1.4.1)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.48.0->pyts) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba>=0.48.0->pyts) (49.2.0)\n",
            "Installing collected packages: pyts\n",
            "Successfully installed pyts-0.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HL_VRTd4akUV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "5a9db54f-fcb3-47f0-85fe-8ca252c2dff1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "329tnhD4W-sK",
        "colab_type": "text"
      },
      "source": [
        "# tsfile loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PL9H-T7rV_4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class TsFileParseException(Exception):\n",
        "    \"\"\"\n",
        "    Should be raised when parsing a .ts file and the format is incorrect.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "def load_from_tsfile_to_dataframe(path_to_file, return_separate_X_and_y=True,\n",
        "                                  replace_missing_vals_with='NaN'):\n",
        "    \"\"\"Loads data from a .ts file into a Pandas DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path_to_file: str\n",
        "        The full pathname of the .ts file to read.\n",
        "    return_separate_X_and_y: bool\n",
        "        true if X and Y values should be returned as separate Data Frames (X) and a numpy array (y), false otherwise.\n",
        "        This is only relevant for data that\n",
        "    replace_missing_vals_with: str\n",
        "       The value that missing values in the text file should be replaced with prior to parsing.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    DataFrame, ndarray\n",
        "        If return_separate_X_and_y then a tuple containing a DataFrame and a numpy array containing the relevant time-series and corresponding class values.\n",
        "    DataFrame\n",
        "        If not return_separate_X_and_y then a single DataFrame containing all time-series and (if relevant) a column \"class_vals\" the associated class values.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize flags and variables used when parsing the file\n",
        "    metadata_started = False\n",
        "    data_started = False\n",
        "\n",
        "    has_problem_name_tag = False\n",
        "    has_timestamps_tag = False\n",
        "    has_univariate_tag = False\n",
        "    has_class_labels_tag = False\n",
        "    has_target_labels_tag = False\n",
        "    has_data_tag = False\n",
        "\n",
        "    previous_timestamp_was_float = None\n",
        "    previous_timestamp_was_int = None\n",
        "    previous_timestamp_was_timestamp = None\n",
        "    num_dimensions = None\n",
        "    is_first_case = True\n",
        "    instance_list = []\n",
        "    class_val_list = []\n",
        "    line_num = 0\n",
        "\n",
        "    # Parse the file\n",
        "    with open(path_to_file, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            # Strip white space from start/end of line and change to lowercase for use below\n",
        "            line = line.strip().lower()\n",
        "            # Empty lines are valid at any point in a file\n",
        "            if line:\n",
        "                # Check if this line contains metadata\n",
        "                # Please note that even though metadata is stored in this function it is not currently published externally\n",
        "                if line.startswith(\"@problemname\"):\n",
        "                    # Check that the data has not started\n",
        "                    if data_started:\n",
        "                        raise TsFileParseException(\"metadata must come before data\")\n",
        "                    # Check that the associated value is valid\n",
        "                    tokens = line.split(' ')\n",
        "                    token_len = len(tokens)\n",
        "\n",
        "                    if token_len == 1:\n",
        "                        raise TsFileParseException(\"problemname tag requires an associated value\")\n",
        "\n",
        "                    problem_name = line[len(\"@problemname\") + 1:]\n",
        "                    has_problem_name_tag = True\n",
        "                    metadata_started = True\n",
        "                elif line.startswith(\"@timestamps\"):\n",
        "                    # Check that the data has not started\n",
        "                    if data_started:\n",
        "                        raise TsFileParseException(\"metadata must come before data\")\n",
        "\n",
        "                    # Check that the associated value is valid\n",
        "                    tokens = line.split(' ')\n",
        "                    token_len = len(tokens)\n",
        "\n",
        "                    if token_len != 2:\n",
        "                        raise TsFileParseException(\"timestamps tag requires an associated Boolean value\")\n",
        "                    elif tokens[1] == \"true\":\n",
        "                        timestamps = True\n",
        "                    elif tokens[1] == \"false\":\n",
        "                        timestamps = False\n",
        "                    else:\n",
        "                        raise TsFileParseException(\"invalid timestamps value\")\n",
        "                    has_timestamps_tag = True\n",
        "                    metadata_started = True\n",
        "                elif line.startswith(\"@univariate\"):\n",
        "                    # Check that the data has not started\n",
        "                    if data_started:\n",
        "                        raise TsFileParseException(\"metadata must come before data\")\n",
        "\n",
        "                    # Check that the associated value is valid\n",
        "                    tokens = line.split(' ')\n",
        "                    token_len = len(tokens)\n",
        "                    if token_len != 2:\n",
        "                        raise TsFileParseException(\"univariate tag requires an associated Boolean value\")\n",
        "                    elif tokens[1] == \"true\":\n",
        "                        univariate = True\n",
        "                    elif tokens[1] == \"false\":\n",
        "                        univariate = False\n",
        "                    else:\n",
        "                        raise TsFileParseException(\"invalid univariate value\")\n",
        "\n",
        "                    has_univariate_tag = True\n",
        "                    metadata_started = True\n",
        "                elif line.startswith(\"@classlabel\"):\n",
        "                    # Check that the data has not started\n",
        "                    if data_started:\n",
        "                        raise TsFileParseException(\"metadata must come before data\")\n",
        "\n",
        "                    # Check that the associated value is valid\n",
        "                    tokens = line.split(' ')\n",
        "                    token_len = len(tokens)\n",
        "\n",
        "                    if token_len == 1:\n",
        "                        raise TsFileParseException(\"classlabel tag requires an associated Boolean value\")\n",
        "\n",
        "                    if tokens[1] == \"true\":\n",
        "                        class_labels = True\n",
        "                    elif tokens[1] == \"false\":\n",
        "                        class_labels = False\n",
        "                    else:\n",
        "                        raise TsFileParseException(\"invalid classLabel value\")\n",
        "\n",
        "                    # Check if we have any associated class values\n",
        "                    if token_len == 2 and class_labels:\n",
        "                        raise TsFileParseException(\"if the classlabel tag is true then class values must be supplied\")\n",
        "\n",
        "                    has_class_labels_tag = True\n",
        "                    class_label_list = [token.strip() for token in tokens[2:]]\n",
        "                    metadata_started = True\n",
        "                elif line.startswith(\"@targetlabel\"):\n",
        "                    # Check that the data has not started\n",
        "                    if data_started:\n",
        "                        raise TsFileParseException(\"metadata must come before data\")\n",
        "\n",
        "                    # Check that the associated value is valid\n",
        "                    tokens = line.split(' ')\n",
        "                    token_len = len(tokens)\n",
        "\n",
        "                    if token_len == 1:\n",
        "                        raise TsFileParseException(\"targetlabel tag requires an associated Boolean value\")\n",
        "\n",
        "                    if tokens[1] == \"true\":\n",
        "                        target_labels = True\n",
        "                    elif tokens[1] == \"false\":\n",
        "                        target_labels = False\n",
        "                    else:\n",
        "                        raise TsFileParseException(\"invalid targetLabel value\")\n",
        "\n",
        "                    has_target_labels_tag = True\n",
        "                    class_val_list = []\n",
        "                    metadata_started = True\n",
        "                # Check if this line contains the start of data\n",
        "                elif line.startswith(\"@data\"):\n",
        "                    if line != \"@data\":\n",
        "                        raise TsFileParseException(\"data tag should not have an associated value\")\n",
        "\n",
        "                    if data_started and not metadata_started:\n",
        "                        raise TsFileParseException(\"metadata must come before data\")\n",
        "                    else:\n",
        "                        has_data_tag = True\n",
        "                        data_started = True\n",
        "                # If the 'data tag has been found then metadata has been parsed and data can be loaded\n",
        "                elif data_started:\n",
        "                    # Check that a full set of metadata has been provided\n",
        "                    incomplete_regression_meta_data = not has_problem_name_tag or not has_timestamps_tag or not has_univariate_tag or not has_target_labels_tag or not has_data_tag\n",
        "                    incomplete_classification_meta_data = not has_problem_name_tag or not has_timestamps_tag or not has_univariate_tag or not has_class_labels_tag or not has_data_tag\n",
        "                    if incomplete_regression_meta_data and incomplete_classification_meta_data:\n",
        "                        raise TsFileParseException(\"a full set of metadata has not been provided before the data\")\n",
        "\n",
        "                    # Replace any missing values with the value specified\n",
        "                    line = line.replace(\"?\", replace_missing_vals_with)\n",
        "\n",
        "                    # Check if we dealing with data that has timestamps\n",
        "                    if timestamps:\n",
        "                        # We're dealing with timestamps so cannot just split line on ':' as timestamps may contain one\n",
        "                        has_another_value = False\n",
        "                        has_another_dimension = False\n",
        "\n",
        "                        timestamps_for_dimension = []\n",
        "                        values_for_dimension = []\n",
        "\n",
        "                        this_line_num_dimensions = 0\n",
        "                        line_len = len(line)\n",
        "                        char_num = 0\n",
        "\n",
        "                        while char_num < line_len:\n",
        "                            # Move through any spaces\n",
        "                            while char_num < line_len and str.isspace(line[char_num]):\n",
        "                                char_num += 1\n",
        "\n",
        "                            # See if there is any more data to read in or if we should validate that read thus far\n",
        "\n",
        "                            if char_num < line_len:\n",
        "\n",
        "                                # See if we have an empty dimension (i.e. no values)\n",
        "                                if line[char_num] == \":\":\n",
        "                                    if len(instance_list) < (this_line_num_dimensions + 1):\n",
        "                                        instance_list.append([])\n",
        "\n",
        "                                    instance_list[this_line_num_dimensions].append(pd.Series())\n",
        "                                    this_line_num_dimensions += 1\n",
        "\n",
        "                                    has_another_value = False\n",
        "                                    has_another_dimension = True\n",
        "\n",
        "                                    timestamps_for_dimension = []\n",
        "                                    values_for_dimension = []\n",
        "\n",
        "                                    char_num += 1\n",
        "                                else:\n",
        "                                    # Check if we have reached a class label\n",
        "                                    if line[char_num] != \"(\" and target_labels:\n",
        "                                        class_val = line[char_num:].strip()\n",
        "\n",
        "                                        # if class_val not in class_val_list:\n",
        "                                        #     raise TsFileParseException(\n",
        "                                        #         \"the class value '\" + class_val + \"' on line \" + str(\n",
        "                                        #             line_num + 1) + \" is not valid\")\n",
        "\n",
        "                                        class_val_list.append(float(class_val))\n",
        "                                        char_num = line_len\n",
        "\n",
        "                                        has_another_value = False\n",
        "                                        has_another_dimension = False\n",
        "\n",
        "                                        timestamps_for_dimension = []\n",
        "                                        values_for_dimension = []\n",
        "\n",
        "                                    else:\n",
        "\n",
        "                                        # Read in the data contained within the next tuple\n",
        "\n",
        "                                        if line[char_num] != \"(\" and not target_labels:\n",
        "                                            raise TsFileParseException(\n",
        "                                                \"dimension \" + str(this_line_num_dimensions + 1) + \" on line \" + str(\n",
        "                                                    line_num + 1) + \" does not start with a '('\")\n",
        "\n",
        "                                        char_num += 1\n",
        "                                        tuple_data = \"\"\n",
        "\n",
        "                                        while char_num < line_len and line[char_num] != \")\":\n",
        "                                            tuple_data += line[char_num]\n",
        "                                            char_num += 1\n",
        "\n",
        "                                        if char_num >= line_len or line[char_num] != \")\":\n",
        "                                            raise TsFileParseException(\n",
        "                                                \"dimension \" + str(this_line_num_dimensions + 1) + \" on line \" + str(\n",
        "                                                    line_num + 1) + \" does not end with a ')'\")\n",
        "\n",
        "                                        # Read in any spaces immediately after the current tuple\n",
        "\n",
        "                                        char_num += 1\n",
        "\n",
        "                                        while char_num < line_len and str.isspace(line[char_num]):\n",
        "                                            char_num += 1\n",
        "\n",
        "                                        # Check if there is another value or dimension to process after this tuple\n",
        "\n",
        "                                        if char_num >= line_len:\n",
        "                                            has_another_value = False\n",
        "                                            has_another_dimension = False\n",
        "\n",
        "                                        elif line[char_num] == \",\":\n",
        "                                            has_another_value = True\n",
        "                                            has_another_dimension = False\n",
        "\n",
        "                                        elif line[char_num] == \":\":\n",
        "                                            has_another_value = False\n",
        "                                            has_another_dimension = True\n",
        "\n",
        "                                        char_num += 1\n",
        "\n",
        "                                        # Get the numeric value for the tuple by reading from the end of the tuple data backwards to the last comma\n",
        "\n",
        "                                        last_comma_index = tuple_data.rfind(',')\n",
        "\n",
        "                                        if last_comma_index == -1:\n",
        "                                            raise TsFileParseException(\n",
        "                                                \"dimension \" + str(this_line_num_dimensions + 1) + \" on line \" + str(\n",
        "                                                    line_num + 1) + \" contains a tuple that has no comma inside of it\")\n",
        "\n",
        "                                        try:\n",
        "                                            value = tuple_data[last_comma_index + 1:]\n",
        "                                            value = float(value)\n",
        "\n",
        "                                        except ValueError:\n",
        "                                            raise TsFileParseException(\n",
        "                                                \"dimension \" + str(this_line_num_dimensions + 1) + \" on line \" + str(\n",
        "                                                    line_num + 1) + \" contains a tuple that does not have a valid numeric value\")\n",
        "\n",
        "                                        # Check the type of timestamp that we have\n",
        "\n",
        "                                        timestamp = tuple_data[0: last_comma_index]\n",
        "\n",
        "                                        try:\n",
        "                                            timestamp = int(timestamp)\n",
        "                                            timestamp_is_int = True\n",
        "                                            timestamp_is_timestamp = False\n",
        "                                        except ValueError:\n",
        "                                            timestamp_is_int = False\n",
        "\n",
        "                                        if not timestamp_is_int:\n",
        "                                            try:\n",
        "                                                timestamp = float(timestamp)\n",
        "                                                timestamp_is_float = True\n",
        "                                                timestamp_is_timestamp = False\n",
        "                                            except ValueError:\n",
        "                                                timestamp_is_float = False\n",
        "\n",
        "                                        if not timestamp_is_int and not timestamp_is_float:\n",
        "                                            try:\n",
        "                                                timestamp = timestamp.strip()\n",
        "                                                timestamp_is_timestamp = True\n",
        "                                            except ValueError:\n",
        "                                                timestamp_is_timestamp = False\n",
        "\n",
        "                                        # Make sure that the timestamps in the file (not just this dimension or case) are consistent\n",
        "\n",
        "                                        if not timestamp_is_timestamp and not timestamp_is_int and not timestamp_is_float:\n",
        "                                            raise TsFileParseException(\n",
        "                                                \"dimension \" + str(this_line_num_dimensions + 1) + \" on line \" + str(\n",
        "                                                    line_num + 1) + \" contains a tuple that has an invalid timestamp '\" + timestamp + \"'\")\n",
        "\n",
        "                                        if previous_timestamp_was_float is not None and previous_timestamp_was_float and not timestamp_is_float:\n",
        "                                            raise TsFileParseException(\n",
        "                                                \"dimension \" + str(this_line_num_dimensions + 1) + \" on line \" + str(\n",
        "                                                    line_num + 1) + \" contains tuples where the timestamp format is inconsistent\")\n",
        "\n",
        "                                        if previous_timestamp_was_int is not None and previous_timestamp_was_int and not timestamp_is_int:\n",
        "                                            raise TsFileParseException(\n",
        "                                                \"dimension \" + str(this_line_num_dimensions + 1) + \" on line \" + str(\n",
        "                                                    line_num + 1) + \" contains tuples where the timestamp format is inconsistent\")\n",
        "\n",
        "                                        if previous_timestamp_was_timestamp is not None and previous_timestamp_was_timestamp and not timestamp_is_timestamp:\n",
        "                                            raise TsFileParseException(\n",
        "                                                \"dimension \" + str(this_line_num_dimensions + 1) + \" on line \" + str(\n",
        "                                                    line_num + 1) + \" contains tuples where the timestamp format is inconsistent\")\n",
        "\n",
        "                                        # Store the values\n",
        "\n",
        "                                        timestamps_for_dimension += [timestamp]\n",
        "                                        values_for_dimension += [value]\n",
        "\n",
        "                                        #  If this was our first tuple then we store the type of timestamp we had\n",
        "\n",
        "                                        if previous_timestamp_was_timestamp is None and timestamp_is_timestamp:\n",
        "                                            previous_timestamp_was_timestamp = True\n",
        "                                            previous_timestamp_was_int = False\n",
        "                                            previous_timestamp_was_float = False\n",
        "\n",
        "                                        if previous_timestamp_was_int is None and timestamp_is_int:\n",
        "                                            previous_timestamp_was_timestamp = False\n",
        "                                            previous_timestamp_was_int = True\n",
        "                                            previous_timestamp_was_float = False\n",
        "\n",
        "                                        if previous_timestamp_was_float is None and timestamp_is_float:\n",
        "                                            previous_timestamp_was_timestamp = False\n",
        "                                            previous_timestamp_was_int = False\n",
        "                                            previous_timestamp_was_float = True\n",
        "\n",
        "                                        # See if we should add the data for this dimension\n",
        "\n",
        "                                        if not has_another_value:\n",
        "                                            if len(instance_list) < (this_line_num_dimensions + 1):\n",
        "                                                instance_list.append([])\n",
        "\n",
        "                                            if timestamp_is_timestamp:\n",
        "                                                timestamps_for_dimension = pd.DatetimeIndex(timestamps_for_dimension)\n",
        "\n",
        "                                            instance_list[this_line_num_dimensions].append(\n",
        "                                                pd.Series(index=timestamps_for_dimension, data=values_for_dimension))\n",
        "                                            this_line_num_dimensions += 1\n",
        "\n",
        "                                            timestamps_for_dimension = []\n",
        "                                            values_for_dimension = []\n",
        "\n",
        "                            elif has_another_value:\n",
        "                                raise TsFileParseException(\n",
        "                                    \"dimension \" + str(this_line_num_dimensions + 1) + \" on line \" + str(\n",
        "                                        line_num + 1) + \" ends with a ',' that is not followed by another tuple\")\n",
        "\n",
        "                            elif has_another_dimension and target_labels:\n",
        "                                raise TsFileParseException(\n",
        "                                    \"dimension \" + str(this_line_num_dimensions + 1) + \" on line \" + str(\n",
        "                                        line_num + 1) + \" ends with a ':' while it should list a class value\")\n",
        "\n",
        "                            elif has_another_dimension and not target_labels:\n",
        "                                if len(instance_list) < (this_line_num_dimensions + 1):\n",
        "                                    instance_list.append([])\n",
        "\n",
        "                                instance_list[this_line_num_dimensions].append(pd.Series(dtype=np.float32))\n",
        "                                this_line_num_dimensions += 1\n",
        "                                num_dimensions = this_line_num_dimensions\n",
        "\n",
        "                            # If this is the 1st line of data we have seen then note the dimensions\n",
        "\n",
        "                            if not has_another_value and not has_another_dimension:\n",
        "                                if num_dimensions is None:\n",
        "                                    num_dimensions = this_line_num_dimensions\n",
        "\n",
        "                                if num_dimensions != this_line_num_dimensions:\n",
        "                                    raise TsFileParseException(\"line \" + str(\n",
        "                                        line_num + 1) + \" does not have the same number of dimensions as the previous line of data\")\n",
        "\n",
        "                        # Check that we are not expecting some more data, and if not, store that processed above\n",
        "\n",
        "                        if has_another_value:\n",
        "                            raise TsFileParseException(\n",
        "                                \"dimension \" + str(this_line_num_dimensions + 1) + \" on line \" + str(\n",
        "                                    line_num + 1) + \" ends with a ',' that is not followed by another tuple\")\n",
        "\n",
        "                        elif has_another_dimension and target_labels:\n",
        "                            raise TsFileParseException(\n",
        "                                \"dimension \" + str(this_line_num_dimensions + 1) + \" on line \" + str(\n",
        "                                    line_num + 1) + \" ends with a ':' while it should list a class value\")\n",
        "\n",
        "                        elif has_another_dimension and not target_labels:\n",
        "                            if len(instance_list) < (this_line_num_dimensions + 1):\n",
        "                                instance_list.append([])\n",
        "\n",
        "                            instance_list[this_line_num_dimensions].append(pd.Series())\n",
        "                            this_line_num_dimensions += 1\n",
        "                            num_dimensions = this_line_num_dimensions\n",
        "\n",
        "                        # If this is the 1st line of data we have seen then note the dimensions\n",
        "\n",
        "                        if not has_another_value and num_dimensions != this_line_num_dimensions:\n",
        "                            raise TsFileParseException(\"line \" + str(\n",
        "                                line_num + 1) + \" does not have the same number of dimensions as the previous line of data\")\n",
        "\n",
        "                        # Check if we should have class values, and if so that they are contained in those listed in the metadata\n",
        "\n",
        "                        if target_labels and len(class_val_list) == 0:\n",
        "                            raise TsFileParseException(\"the cases have no associated class values\")\n",
        "                    else:\n",
        "                        dimensions = line.split(\":\")\n",
        "                        # If first row then note the number of dimensions (that must be the same for all cases)\n",
        "                        if is_first_case:\n",
        "                            num_dimensions = len(dimensions)\n",
        "\n",
        "                            if target_labels:\n",
        "                                num_dimensions -= 1\n",
        "\n",
        "                            for dim in range(0, num_dimensions):\n",
        "                                instance_list.append([])\n",
        "                            is_first_case = False\n",
        "\n",
        "                        # See how many dimensions that the case whose data in represented in this line has\n",
        "                        this_line_num_dimensions = len(dimensions)\n",
        "\n",
        "                        if target_labels:\n",
        "                            this_line_num_dimensions -= 1\n",
        "\n",
        "                        # All dimensions should be included for all series, even if they are empty\n",
        "                        if this_line_num_dimensions != num_dimensions:\n",
        "                            raise TsFileParseException(\"inconsistent number of dimensions. Expecting \" + str(\n",
        "                                num_dimensions) + \" but have read \" + str(this_line_num_dimensions))\n",
        "\n",
        "                        # Process the data for each dimension\n",
        "                        for dim in range(0, num_dimensions):\n",
        "                            dimension = dimensions[dim].strip()\n",
        "\n",
        "                            if dimension:\n",
        "                                data_series = dimension.split(\",\")\n",
        "                                data_series = [float(i) for i in data_series]\n",
        "                                instance_list[dim].append(pd.Series(data_series))\n",
        "                            else:\n",
        "                                instance_list[dim].append(pd.Series())\n",
        "\n",
        "                        if target_labels:\n",
        "                            class_val_list.append(float(dimensions[num_dimensions].strip()))\n",
        "\n",
        "            line_num += 1\n",
        "\n",
        "    # Check that the file was not empty\n",
        "    if line_num:\n",
        "        # Check that the file contained both metadata and data\n",
        "        complete_regression_meta_data = has_problem_name_tag and has_timestamps_tag and has_univariate_tag and has_target_labels_tag and has_data_tag\n",
        "        complete_classification_meta_data = has_problem_name_tag and has_timestamps_tag and has_univariate_tag and has_class_labels_tag and has_data_tag\n",
        "\n",
        "        if metadata_started and not complete_regression_meta_data and not complete_classification_meta_data:\n",
        "            raise TsFileParseException(\"metadata incomplete\")\n",
        "        elif metadata_started and not data_started:\n",
        "            raise TsFileParseException(\"file contained metadata but no data\")\n",
        "        elif metadata_started and data_started and len(instance_list) == 0:\n",
        "            raise TsFileParseException(\"file contained metadata but no data\")\n",
        "\n",
        "        # Create a DataFrame from the data parsed above\n",
        "        data = pd.DataFrame(dtype=np.float32)\n",
        "\n",
        "        for dim in range(0, num_dimensions):\n",
        "            data['dim_' + str(dim)] = instance_list[dim]\n",
        "\n",
        "        # Check if we should return any associated class labels separately\n",
        "        if target_labels:\n",
        "            if return_separate_X_and_y:\n",
        "                return data, np.asarray(class_val_list)\n",
        "            else:\n",
        "                data['class_vals'] = pd.Series(class_val_list)\n",
        "                return data\n",
        "        else:\n",
        "            return data\n",
        "    else:\n",
        "        raise TsFileParseException(\"empty file\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZedZWbPrXxJd",
        "colab_type": "text"
      },
      "source": [
        "#Data Loader\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVMZ1gdvHRA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pmdarima as pm\n",
        "from pyts.approximation import PiecewiseAggregateApproximation\n",
        "from pyts.image import RecurrencePlot\n",
        "import time\n",
        "\n",
        "\n",
        "class DataLoader:\n",
        "    def __init__(self, path_to_file):\n",
        "        self.data, _ = load_from_tsfile_to_dataframe(path_to_file=path_to_file)\n",
        "        self.num_instances, self.num_variables = self.data.shape\n",
        "\n",
        "        # Use first 20k samples only to save memory\n",
        "        if self.num_instances > 20000:\n",
        "            self.data = self.data.iloc[:20000]\n",
        "            self.num_instances = 20000\n",
        "\n",
        "        # Get (max) time series length.\n",
        "        # Handle case where variables do not have the same lengths by taking the max\n",
        "        # Shorter ones are padded\n",
        "        self.ts_length = max([len(self.data.iloc[0][i]) for i in range(self.num_variables)])\n",
        "\n",
        "        # If no. of observations > 2000, use the first 2000 observations only\n",
        "        self.ts_length = self.ts_length if self.ts_length <= 2000 else 2000\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"\n",
        "        Prepare numpy array X with shape (num_instances, ts_length, num_variables)\n",
        "        and Y with shape (num_instances, num_variables)\n",
        "        \"\"\"\n",
        "\n",
        "        # Decrement by 1 since observation s_j, 1 <= j <= t is split as such:\n",
        "        # X_i = [s_1,...,s_t-1], Y_i = s_t\n",
        "        X, Y = np.empty((self.num_instances, self.ts_length - 1, self.num_variables)), \\\n",
        "               np.empty((self.num_instances, self.num_variables))\n",
        "\n",
        "        # For all instance\n",
        "        start = time.time()\n",
        "        for idx, row in enumerate(self.data.iterrows()):\n",
        "            for i in range(self.num_variables):\n",
        "                # Get current variable's series\n",
        "                # Apply linear interpolation on missing values\n",
        "                # Handle case when no. observations > 2000 by enforcing a length slice\n",
        "                s = row[1][i].interpolate(limit_direction='both').to_numpy()[:self.ts_length]\n",
        "\n",
        "                # Case when a variable's series has a shorter length\n",
        "                if s.size != self.ts_length:\n",
        "                    # Pad beginning with zeros\n",
        "                    s = np.pad(s, (self.ts_length - s.size, 0), 'constant', constant_values=0.)\n",
        "\n",
        "                X[idx, :, i] = s[:-1]\n",
        "                Y[idx, i] = s[-1]\n",
        "        end = time.time()\n",
        "        # print(f\"Data loaded in {end - start} seconds\")\n",
        "\n",
        "        # Free data variable\n",
        "        self.data = None\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "    def get_residuals(self):\n",
        "        \"\"\"\n",
        "        Get ARIMA residuals of each variable. Used for BDS tests\n",
        "        \"\"\"\n",
        "        # Get time series length.\n",
        "        # Handle case where variables do not have the same lengths by taking the max\n",
        "        # Shorter ones are padded\n",
        "        self.ts_length = max([len(self.data.iloc[0][i]) for i in range(self.num_variables)])\n",
        "\n",
        "        residuals = np.empty((self.num_variables, self.ts_length))\n",
        "\n",
        "        # Take a sample. For each variable\n",
        "        for i in range(self.num_variables):\n",
        "            # Obtain variable's time series\n",
        "            sample = self.data.iloc[0][i].interpolate(limit_direction='both').to_numpy()\n",
        "\n",
        "            # Fit arima and obtain residuals\n",
        "            model = pm.auto_arima(sample, seasonal=False, start_p=2, max_p=10, max_d=10, max_q=10)\n",
        "            res = np.array(model.resid())\n",
        "\n",
        "            # Case when a variable's series has a shorter length\n",
        "            if res.size != self.ts_length:\n",
        "                # Pad beginning with zeros\n",
        "                res = np.pad(res, (self.ts_length - res.size, 0), 'constant', constant_values=0.)\n",
        "\n",
        "            residuals[i] = res\n",
        "\n",
        "        return residuals\n",
        "\n",
        "\n",
        "class CNNDataLoader(DataLoader):\n",
        "    def __init__(self, path_to_file, img_size):\n",
        "        super().__init__(path_to_file)\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"\n",
        "        Prepare numpy array X with shape (num_instances, img_size, img_size, num_variables)\n",
        "        and y with shape (num_instances, num_variables)\n",
        "        \"\"\"\n",
        "\n",
        "        X, Y = np.empty((self.num_instances, self.img_size, self.img_size, self.num_variables)), \\\n",
        "               np.empty((self.num_instances, self.num_variables))\n",
        "        # print(X.shape)\n",
        "\n",
        "        # Initialize PAA transformer\n",
        "        paa = PiecewiseAggregateApproximation(window_size=None, output_size=self.img_size, overlapping=False)\n",
        "        rp = RecurrencePlot()\n",
        "\n",
        "        # For all instance\n",
        "        start = time.time()\n",
        "        for idx, row in enumerate(self.data.iterrows()):\n",
        "            for i in range(self.num_variables):\n",
        "                # Get current variable's series\n",
        "                # Apply linear interpolation on missing values\n",
        "                # Handle case when no. observations > 2000 by enforcing a length slice\n",
        "                s = row[1][i].interpolate(limit_direction='both').to_numpy()[:self.ts_length]\n",
        "\n",
        "                # Case when a variable's series has a shorter length\n",
        "                if s.size != self.ts_length:\n",
        "                    # Pad beginning with zeros\n",
        "                    s = np.pad(s, (self.ts_length - s.size, 0), 'constant', constant_values=0.)\n",
        "\n",
        "                # Apply PAA and RP\n",
        "                X[idx, :, :, i] = rp.transform(paa.transform(np.expand_dims(s[:-1], axis=0)))[0]\n",
        "                Y[idx, i] = s[-1]\n",
        "        end = time.time()\n",
        "        # print(f\"Data loaded in {end - start} seconds\")\n",
        "\n",
        "        return X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtOgNL5zYmZl",
        "colab_type": "text"
      },
      "source": [
        "#Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7gF2XakYrGO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7df70cef-970f-4b43-d9ba-dbd516c14000"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, Input\n",
        "from tensorflow.keras.layers import Conv1D, Conv2D\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Concatenate, Add, add\n",
        "from tensorflow.keras.layers import GlobalAveragePooling1D, MaxPool1D, MaxPooling2D\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "use_tpu = True\n",
        "\n",
        "if use_tpu:\n",
        "    assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  TF_MASTER = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "else:\n",
        "  TF_MASTER=''\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(TF_MASTER)\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "\n",
        "class BaseModel:\n",
        "    def __init__(self, ts_length, num_variables, loss, epochs, batch_size, optimizer):\n",
        "        self.name = 'Base'\n",
        "        self.model = None\n",
        "        self.num_variables = num_variables\n",
        "        self.ts_length = ts_length\n",
        "        self.loss = loss\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def fit(self, dataset):\n",
        "        X_train, Y_train = dataset['X_train'], dataset['Y_train']\n",
        "        X_test, Y_test = dataset['X_test'], dataset['Y_test']\n",
        "\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=50, min_lr=0.0001)\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=200)\n",
        "\n",
        "        # Train model\n",
        "        self.model.fit(X_train, Y_train, epochs=self.epochs, batch_size=self.batch_size*8,\n",
        "                       verbose=0, validation_split=0.2, callbacks=[reduce_lr, early_stopping])\n",
        "        _, rmse = self.model.evaluate(X_test, Y_test)\n",
        "\n",
        "        return rmse\n",
        "\n",
        "    def build_model(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class CNNModel(BaseModel):\n",
        "    def __init__(self, img_size, num_variables, loss, epochs, batch_size, optimizer):\n",
        "        super().__init__(ts_length=None,\n",
        "                         num_variables=num_variables,\n",
        "                         loss=loss,\n",
        "                         epochs=epochs,\n",
        "                         batch_size=batch_size,\n",
        "                         optimizer=optimizer)\n",
        "        self.img_size = img_size\n",
        "        self.name = 'CNN'\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        with strategy.scope():\n",
        "            height, width, n_channels = self.img_size, self.img_size, self.num_variables\n",
        "\n",
        "            model = Sequential([\n",
        "                Conv2D(filters=32, kernel_size=3, padding='same', input_shape=(height, width, n_channels)),\n",
        "                BatchNormalization(),\n",
        "                Activation('relu'),\n",
        "                MaxPooling2D(pool_size=2),\n",
        "                Conv2D(filters=32, kernel_size=3, padding='same'),\n",
        "                BatchNormalization(),\n",
        "                Activation('relu'),\n",
        "                MaxPooling2D(pool_size=2),\n",
        "                Dropout(0.25),\n",
        "                Flatten(),\n",
        "                Dense(256, activation='relu'),\n",
        "                Dropout(0.25),\n",
        "                Dense(self.num_variables, activation='linear')\n",
        "            ])\n",
        "\n",
        "            model.compile(loss=self.loss, optimizer=self.optimizer, metrics=[RootMeanSquaredError()])\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "class FCNModel(BaseModel):\n",
        "    def __init__(self, ts_length, num_variables, loss, epochs, batch_size, optimizer):\n",
        "        super().__init__(ts_length=None,\n",
        "                         num_variables=num_variables,\n",
        "                         loss=loss,\n",
        "                         epochs=epochs,\n",
        "                         batch_size=batch_size,\n",
        "                         optimizer=optimizer)\n",
        "        self.name = 'FCN'\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        with strategy.scope():\n",
        "            input_layer = Input((self.ts_length, self.num_variables))\n",
        "\n",
        "            conv1 = Conv1D(filters=128, kernel_size=8, padding='same')(input_layer)\n",
        "            conv1 = BatchNormalization()(conv1)\n",
        "            conv1 = Activation(activation='relu')(conv1)\n",
        "\n",
        "            conv2 = Conv1D(filters=256, kernel_size=5, padding='same')(conv1)\n",
        "            conv2 = BatchNormalization()(conv2)\n",
        "            conv2 = Activation('relu')(conv2)\n",
        "\n",
        "            conv3 = Conv1D(128, kernel_size=3, padding='same')(conv2)\n",
        "            conv3 = BatchNormalization()(conv3)\n",
        "            conv3 = Activation('relu')(conv3)\n",
        "\n",
        "            gap_layer = GlobalAveragePooling1D()(conv3)\n",
        "\n",
        "            output_layer = Dense(self.num_variables, activation='linear')(gap_layer)\n",
        "\n",
        "            model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "            model.compile(loss=self.loss,\n",
        "                          optimizer=self.optimizer,\n",
        "                          metrics=[RootMeanSquaredError()])\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "class InceptionTimeModel(BaseModel):\n",
        "    def __init__(self, ts_length, num_variables, loss, epochs, batch_size, optimizer):\n",
        "        super().__init__(ts_length=None,\n",
        "                         num_variables=num_variables,\n",
        "                         loss=loss,\n",
        "                         epochs=epochs,\n",
        "                         batch_size=batch_size,\n",
        "                         optimizer=optimizer)\n",
        "        self.name = 'InceptionTime'\n",
        "        self.nb_filters = 32\n",
        "        self.use_residual = True\n",
        "        self.use_bottleneck = True\n",
        "        self.depth = 6\n",
        "        self.kernel_size = 40\n",
        "        self.bottleneck_size = 32\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def _inception_module(self, input_tensor, stride=1, activation='linear'):\n",
        "        if self.use_bottleneck and int(input_tensor.shape[-1]) > 1:\n",
        "            input_inception = Conv1D(filters=self.bottleneck_size, kernel_size=1,\n",
        "                                                  padding='same', activation=activation, use_bias=False)(input_tensor)\n",
        "        else:\n",
        "            input_inception = input_tensor\n",
        "\n",
        "        # kernel_size_s = [3, 5, 8, 11, 17]\n",
        "        kernel_size_s = [self.kernel_size // (2 ** i) for i in range(3)]\n",
        "\n",
        "        conv_list = []\n",
        "\n",
        "        for i in range(len(kernel_size_s)):\n",
        "            conv_list.append(Conv1D(filters=self.nb_filters, kernel_size=kernel_size_s[i],\n",
        "                                                 strides=stride, padding='same', activation=activation, use_bias=False)(\n",
        "                input_inception))\n",
        "\n",
        "        max_pool_1 = MaxPool1D(pool_size=3, strides=stride, padding='same')(input_tensor)\n",
        "\n",
        "        conv_6 = Conv1D(filters=self.nb_filters, kernel_size=1,\n",
        "                                     padding='same', activation=activation, use_bias=False)(max_pool_1)\n",
        "\n",
        "        conv_list.append(conv_6)\n",
        "\n",
        "        x = Concatenate(axis=2)(conv_list)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation(activation='relu')(x)\n",
        "        return x\n",
        "\n",
        "    def _shortcut_layer(self, input_tensor, out_tensor):\n",
        "        shortcut_y = Conv1D(filters=int(out_tensor.shape[-1]), kernel_size=1,\n",
        "                                         padding='same', use_bias=False)(input_tensor)\n",
        "        shortcut_y = BatchNormalization()(shortcut_y)\n",
        "\n",
        "        x = Add()([shortcut_y, out_tensor])\n",
        "        x = Activation('relu')(x)\n",
        "        return x\n",
        "\n",
        "    def build_model(self):\n",
        "        with strategy.scope():\n",
        "            input_layer = Input((self.ts_length, self.num_variables))\n",
        "\n",
        "            x = input_layer\n",
        "            input_res = input_layer\n",
        "\n",
        "            for d in range(self.depth):\n",
        "                x = self._inception_module(x)\n",
        "                if self.use_residual and d % 3 == 2:\n",
        "                    x = self._shortcut_layer(input_res, x)\n",
        "                    input_res = x\n",
        "\n",
        "            gap_layer = GlobalAveragePooling1D()(x)\n",
        "            output_layer = Dense(self.num_variables, activation='linear')(gap_layer)\n",
        "\n",
        "            model = Model(inputs=input_layer, outputs=output_layer)\n",
        "            model.compile(loss=self.loss,\n",
        "                          optimizer=self.optimizer,\n",
        "                          metrics=[RootMeanSquaredError()])\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "class ResNetModel(BaseModel):\n",
        "    def __init__(self, ts_length, num_variables, loss, epochs, batch_size, optimizer):\n",
        "        super().__init__(ts_length=None,\n",
        "                         num_variables=num_variables,\n",
        "                         loss=loss,\n",
        "                         epochs=epochs,\n",
        "                         batch_size=batch_size,\n",
        "                         optimizer=optimizer)\n",
        "        self.name = 'ResNet'\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        with strategy.scope():\n",
        "            n_feature_maps = 64\n",
        "            input_layer = Input((self.ts_length, self.num_variables))\n",
        "\n",
        "            # BLOCK 1\n",
        "            conv_x = Conv1D(filters=n_feature_maps, kernel_size=8, padding='same')(input_layer)\n",
        "            conv_x = BatchNormalization()(conv_x)\n",
        "            conv_x = Activation('relu')(conv_x)\n",
        "\n",
        "            conv_y = Conv1D(filters=n_feature_maps, kernel_size=5, padding='same')(conv_x)\n",
        "            conv_y = BatchNormalization()(conv_y)\n",
        "            conv_y = Activation('relu')(conv_y)\n",
        "\n",
        "            conv_z = Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_y)\n",
        "            conv_z = BatchNormalization()(conv_z)\n",
        "\n",
        "            # expand channels for the sum\n",
        "            shortcut_y = Conv1D(filters=n_feature_maps, kernel_size=1, padding='same')(input_layer)\n",
        "            shortcut_y = BatchNormalization()(shortcut_y)\n",
        "\n",
        "            output_block_1 = add([shortcut_y, conv_z])\n",
        "            output_block_1 = Activation('relu')(output_block_1)\n",
        "\n",
        "            # BLOCK 2\n",
        "            conv_x = Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_1)\n",
        "            conv_x = BatchNormalization()(conv_x)\n",
        "            conv_x = Activation('relu')(conv_x)\n",
        "\n",
        "            conv_y = Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
        "            conv_y = BatchNormalization()(conv_y)\n",
        "            conv_y = Activation('relu')(conv_y)\n",
        "\n",
        "            conv_z = Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
        "            conv_z = BatchNormalization()(conv_z)\n",
        "\n",
        "            # expand channels for the sum\n",
        "            shortcut_y = Conv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n",
        "            shortcut_y = BatchNormalization()(shortcut_y)\n",
        "\n",
        "            output_block_2 = add([shortcut_y, conv_z])\n",
        "            output_block_2 = Activation('relu')(output_block_2)\n",
        "\n",
        "            # BLOCK 3\n",
        "            conv_x = Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n",
        "            conv_x = BatchNormalization()(conv_x)\n",
        "            conv_x = Activation('relu')(conv_x)\n",
        "\n",
        "            conv_y = Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
        "            conv_y = BatchNormalization()(conv_y)\n",
        "            conv_y = Activation('relu')(conv_y)\n",
        "\n",
        "            conv_z = Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
        "            conv_z = BatchNormalization()(conv_z)\n",
        "\n",
        "            # no need to expand channels because they are equal\n",
        "            shortcut_y = BatchNormalization()(output_block_2)\n",
        "\n",
        "            output_block_3 = add([shortcut_y, conv_z])\n",
        "            output_block_3 = Activation('relu')(output_block_3)\n",
        "\n",
        "            # FINAL\n",
        "            gap_layer = GlobalAveragePooling1D()(output_block_3)\n",
        "            output_layer = Dense(self.num_variables, activation='linear')(gap_layer)\n",
        "\n",
        "            model = Model(inputs=input_layer, outputs=output_layer)\n",
        "            model.compile(loss=self.loss,\n",
        "                          optimizer=self.optimizer,\n",
        "                          metrics=[RootMeanSquaredError()])\n",
        "\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.86.43.42:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.86.43.42:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h88tVt71Zonv",
        "colab_type": "text"
      },
      "source": [
        "#Run Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-Bev1gfZubs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9a973341-b532-4014-fab4-7475db66b5d3"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "import numpy as np\n",
        "import logging\n",
        "import csv\n",
        "import gc\n",
        "\n",
        "NUM_ITERS = 5\n",
        "IMG_SIZE = [16, 32, 64]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    regression_datasets = [\"PPGDalia\", \"BIDMC32HR\"]\n",
        "\n",
        "    for name in regression_datasets:\n",
        "        for curr_size in IMG_SIZE:\n",
        "            data_train = CNNDataLoader(path_to_file=\"/content/gdrive/My Drive/rcnn/dataset/\"\n",
        "                                                    f\"{name}/{name}_TRAIN.ts\", img_size=curr_size)\n",
        "\n",
        "            data_test = CNNDataLoader(path_to_file=\"/content/gdrive/My Drive/rcnn/dataset/\"\n",
        "                                                   f\"{name}/{name}_TEST.ts\", img_size=curr_size)\n",
        "\n",
        "            X_train, Y_train = data_train.load_data()\n",
        "            X_test, Y_test = data_test.load_data()\n",
        "\n",
        "            dataset = {\n",
        "                \"X_train\": X_train, \"Y_train\": Y_train,\n",
        "                \"X_test\": X_test, \"Y_test\": Y_test\n",
        "            }\n",
        "            with open('/content/gdrive/My Drive/rcnn/results/results.csv', 'a', newline='') as file:\n",
        "                writer = csv.writer(file)\n",
        "                for i in range(NUM_ITERS):\n",
        "                    K.clear_session()\n",
        "                    cnn = CNNModel(img_size=curr_size, num_variables=data_train.num_variables,\n",
        "                                   loss=\"mean_squared_error\", epochs=1000, batch_size=128,\n",
        "                                   optimizer=Adam())\n",
        "                    result = np.mean(cnn.fit(dataset))\n",
        "                    del cnn\n",
        "                    gc.collect()\n",
        "\n",
        "                    print(f\"{name},CNN-{curr_size},{i + 1},{result}\")\n",
        "                    writer.writerow([name, f\"CNN-{curr_size}\", i+1, result])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  1/625 [..............................] - ETA: 3:11 - loss: 2431.3931 - root_mean_squared_error: 49.3092WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0038s vs `on_test_batch_end` time: 0.0134s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0038s vs `on_test_batch_end` time: 0.0134s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 10s 15ms/step - loss: 3022.7156 - root_mean_squared_error: 54.9792\n",
            "PPGDalia,CNN-16,1,54.97922897338867\n",
            "  1/625 [..............................] - ETA: 3:13 - loss: 2415.1045 - root_mean_squared_error: 49.1437WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_test_batch_end` time: 0.0129s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_test_batch_end` time: 0.0129s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 9s 15ms/step - loss: 2993.7043 - root_mean_squared_error: 54.7148\n",
            "PPGDalia,CNN-16,2,54.714752197265625\n",
            "  1/625 [..............................] - ETA: 3:16 - loss: 2401.4089 - root_mean_squared_error: 49.0042WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0040s vs `on_test_batch_end` time: 0.0130s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0040s vs `on_test_batch_end` time: 0.0130s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 9s 15ms/step - loss: 3023.8071 - root_mean_squared_error: 54.9892\n",
            "PPGDalia,CNN-16,3,54.989158630371094\n",
            "  1/625 [..............................] - ETA: 3:24 - loss: 2261.4927 - root_mean_squared_error: 47.5552WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0043s vs `on_test_batch_end` time: 0.0102s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0043s vs `on_test_batch_end` time: 0.0102s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 9s 15ms/step - loss: 3055.7490 - root_mean_squared_error: 55.2788\n",
            "PPGDalia,CNN-16,4,55.278831481933594\n",
            "  1/625 [..............................] - ETA: 3:18 - loss: 2612.3125 - root_mean_squared_error: 51.1108WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_test_batch_end` time: 0.0145s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_test_batch_end` time: 0.0145s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 10s 16ms/step - loss: 3034.3264 - root_mean_squared_error: 55.0847\n",
            "PPGDalia,CNN-16,5,55.084716796875\n",
            "  1/625 [..............................] - ETA: 5:27 - loss: 2912.2437 - root_mean_squared_error: 53.9652WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_test_batch_end` time: 0.0151s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_test_batch_end` time: 0.0151s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 10s 16ms/step - loss: 2642.2153 - root_mean_squared_error: 51.4025\n",
            "PPGDalia,CNN-32,1,51.40248107910156\n",
            "  1/625 [..............................] - ETA: 5:18 - loss: 3106.1213 - root_mean_squared_error: 55.7326WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0041s vs `on_test_batch_end` time: 0.0168s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0041s vs `on_test_batch_end` time: 0.0168s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 10s 15ms/step - loss: 2638.3062 - root_mean_squared_error: 51.3644\n",
            "PPGDalia,CNN-32,2,51.364444732666016\n",
            "  1/625 [..............................] - ETA: 5:19 - loss: 2915.7478 - root_mean_squared_error: 53.9977WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0038s vs `on_test_batch_end` time: 0.0129s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0038s vs `on_test_batch_end` time: 0.0129s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 9s 15ms/step - loss: 2677.4785 - root_mean_squared_error: 51.7444\n",
            "PPGDalia,CNN-32,3,51.744354248046875\n",
            "  1/625 [..............................] - ETA: 5:22 - loss: 3198.5964 - root_mean_squared_error: 56.5561WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0035s vs `on_test_batch_end` time: 0.0118s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0035s vs `on_test_batch_end` time: 0.0118s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 10s 15ms/step - loss: 2730.0068 - root_mean_squared_error: 52.2495\n",
            "PPGDalia,CNN-32,4,52.24946594238281\n",
            "  1/625 [..............................] - ETA: 5:21 - loss: 3150.5430 - root_mean_squared_error: 56.1297WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0039s vs `on_test_batch_end` time: 0.0130s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0039s vs `on_test_batch_end` time: 0.0130s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 9s 15ms/step - loss: 2670.2578 - root_mean_squared_error: 51.6745\n",
            "PPGDalia,CNN-32,5,51.674537658691406\n",
            "  1/625 [..............................] - ETA: 9:56 - loss: 1865.2358 - root_mean_squared_error: 43.1884WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0054s vs `on_test_batch_end` time: 0.0132s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0054s vs `on_test_batch_end` time: 0.0132s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 10s 16ms/step - loss: 1910.6078 - root_mean_squared_error: 43.7105\n",
            "PPGDalia,CNN-64,1,43.71049880981445\n",
            "  1/625 [..............................] - ETA: 10:07 - loss: 2171.9888 - root_mean_squared_error: 46.6046WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_test_batch_end` time: 0.0130s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_test_batch_end` time: 0.0130s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 10s 16ms/step - loss: 1891.7629 - root_mean_squared_error: 43.4944\n",
            "PPGDalia,CNN-64,2,43.49440383911133\n",
            "  1/625 [..............................] - ETA: 9:46 - loss: 1901.5310 - root_mean_squared_error: 43.6065WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0049s vs `on_test_batch_end` time: 0.0142s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0049s vs `on_test_batch_end` time: 0.0142s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 10s 16ms/step - loss: 1887.1498 - root_mean_squared_error: 43.4413\n",
            "PPGDalia,CNN-64,3,43.44133758544922\n",
            "  1/625 [..............................] - ETA: 10:00 - loss: 2023.7119 - root_mean_squared_error: 44.9857WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0038s vs `on_test_batch_end` time: 0.0132s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0038s vs `on_test_batch_end` time: 0.0132s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 10s 16ms/step - loss: 1939.6427 - root_mean_squared_error: 44.0414\n",
            "PPGDalia,CNN-64,4,44.0413703918457\n",
            "  1/625 [..............................] - ETA: 10:19 - loss: 2339.9065 - root_mean_squared_error: 48.3726WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_test_batch_end` time: 0.0156s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_test_batch_end` time: 0.0156s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 10s 16ms/step - loss: 1902.6508 - root_mean_squared_error: 43.6194\n",
            "PPGDalia,CNN-64,5,43.619384765625\n",
            " 1/75 [..............................] - ETA: 23s - loss: 0.0419 - root_mean_squared_error: 0.2046WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0019s vs `on_test_batch_end` time: 0.0190s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0019s vs `on_test_batch_end` time: 0.0190s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "75/75 [==============================] - 2s 22ms/step - loss: 0.1219 - root_mean_squared_error: 0.3492\n",
            "BIDMC32HR,CNN-16,1,0.3491670489311218\n",
            " 1/75 [..............................] - ETA: 21s - loss: 0.0414 - root_mean_squared_error: 0.2034WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0019s vs `on_test_batch_end` time: 0.0167s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0019s vs `on_test_batch_end` time: 0.0167s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "75/75 [==============================] - 2s 20ms/step - loss: 0.1372 - root_mean_squared_error: 0.3704\n",
            "BIDMC32HR,CNN-16,2,0.37040603160858154\n",
            " 1/75 [..............................] - ETA: 20s - loss: 0.0330 - root_mean_squared_error: 0.1817WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_test_batch_end` time: 0.0154s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_test_batch_end` time: 0.0154s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "75/75 [==============================] - 2s 20ms/step - loss: 0.1264 - root_mean_squared_error: 0.3555\n",
            "BIDMC32HR,CNN-16,3,0.3555140793323517\n",
            " 1/75 [..............................] - ETA: 20s - loss: 0.0297 - root_mean_squared_error: 0.1724WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_test_batch_end` time: 0.0192s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_test_batch_end` time: 0.0192s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "75/75 [==============================] - 2s 20ms/step - loss: 0.1241 - root_mean_squared_error: 0.3522\n",
            "BIDMC32HR,CNN-16,4,0.3522086441516876\n",
            " 1/75 [..............................] - ETA: 20s - loss: 0.0312 - root_mean_squared_error: 0.1766WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_test_batch_end` time: 0.0145s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_test_batch_end` time: 0.0145s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "75/75 [==============================] - 2s 21ms/step - loss: 0.1252 - root_mean_squared_error: 0.3538\n",
            "BIDMC32HR,CNN-16,5,0.3537915349006653\n",
            " 1/75 [..............................] - ETA: 36s - loss: 0.0454 - root_mean_squared_error: 0.2130WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_test_batch_end` time: 0.0183s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_test_batch_end` time: 0.0183s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "75/75 [==============================] - 2s 23ms/step - loss: 0.1182 - root_mean_squared_error: 0.3437\n",
            "BIDMC32HR,CNN-32,1,0.343741774559021\n",
            " 1/75 [..............................] - ETA: 34s - loss: 0.0349 - root_mean_squared_error: 0.1868WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0031s vs `on_test_batch_end` time: 0.0164s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0031s vs `on_test_batch_end` time: 0.0164s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "75/75 [==============================] - 2s 23ms/step - loss: 0.1193 - root_mean_squared_error: 0.3454\n",
            "BIDMC32HR,CNN-32,2,0.34539490938186646\n",
            " 1/75 [..............................] - ETA: 36s - loss: 0.0296 - root_mean_squared_error: 0.1721WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_test_batch_end` time: 0.0177s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_test_batch_end` time: 0.0177s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "75/75 [==============================] - 2s 22ms/step - loss: 0.1166 - root_mean_squared_error: 0.3415\n",
            "BIDMC32HR,CNN-32,3,0.3415064811706543\n",
            " 1/75 [..............................] - ETA: 34s - loss: 0.0533 - root_mean_squared_error: 0.2309WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_test_batch_end` time: 0.0166s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_test_batch_end` time: 0.0166s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "75/75 [==============================] - 2s 23ms/step - loss: 0.1152 - root_mean_squared_error: 0.3395\n",
            "BIDMC32HR,CNN-32,4,0.33947911858558655\n",
            " 1/75 [..............................] - ETA: 35s - loss: 0.0337 - root_mean_squared_error: 0.1836WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_test_batch_end` time: 0.0175s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_test_batch_end` time: 0.0175s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "75/75 [==============================] - 2s 23ms/step - loss: 0.1188 - root_mean_squared_error: 0.3447\n",
            "BIDMC32HR,CNN-32,5,0.3447003960609436\n",
            " 1/75 [..............................] - ETA: 1:10 - loss: 0.0282 - root_mean_squared_error: 0.1678WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_test_batch_end` time: 0.0161s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_test_batch_end` time: 0.0161s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "75/75 [==============================] - 2s 30ms/step - loss: 0.1914 - root_mean_squared_error: 0.4375\n",
            "BIDMC32HR,CNN-64,1,0.4374832808971405\n",
            " 1/75 [..............................] - ETA: 1:08 - loss: 0.0230 - root_mean_squared_error: 0.1517WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_test_batch_end` time: 0.0185s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_test_batch_end` time: 0.0185s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "75/75 [==============================] - 2s 30ms/step - loss: 0.0969 - root_mean_squared_error: 0.3112\n",
            "BIDMC32HR,CNN-64,2,0.31123000383377075\n",
            " 1/75 [..............................] - ETA: 1:10 - loss: 0.0353 - root_mean_squared_error: 0.1878WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0028s vs `on_test_batch_end` time: 0.0174s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0028s vs `on_test_batch_end` time: 0.0174s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "75/75 [==============================] - 2s 30ms/step - loss: 0.0976 - root_mean_squared_error: 0.3123\n",
            "BIDMC32HR,CNN-64,3,0.31233420968055725\n",
            " 1/75 [..............................] - ETA: 1:08 - loss: 0.0488 - root_mean_squared_error: 0.2209WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_test_batch_end` time: 0.0161s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_test_batch_end` time: 0.0161s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "75/75 [==============================] - 2s 30ms/step - loss: 0.1060 - root_mean_squared_error: 0.3255\n",
            "BIDMC32HR,CNN-64,4,0.32552996277809143\n",
            " 1/75 [..............................] - ETA: 1:08 - loss: 0.0308 - root_mean_squared_error: 0.1755WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0024s vs `on_test_batch_end` time: 0.0186s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0024s vs `on_test_batch_end` time: 0.0186s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "75/75 [==============================] - 2s 30ms/step - loss: 0.1917 - root_mean_squared_error: 0.4378\n",
            "BIDMC32HR,CNN-64,5,0.43781137466430664\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}